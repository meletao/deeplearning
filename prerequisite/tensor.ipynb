{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ada6d5-56bf-4d7e-aff9-e1fe08bea291",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83d56a-1842-4c67-b4e7-650e050b9903",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f968113-1839-4721-af0e-159f69915797",
   "metadata": {},
   "source": [
    "`torch` is a python library that provides all the necessary tools needed for building and training deep neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1eb3b-1e5e-42fc-856a-f4cd79f400d3",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ec948-159b-4439-8046-4a66d3bf5db9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "A `tensor` is the fundamental data structure that `torch` uses to store numerical data and perform numerical operations. `torch` enables us to utilize GPUs for faster computation, which is crucial for deep neural nets. Therefore, it is important to spend sometime to get comfortable with `tensor`. \n",
    "\n",
    "Before going further, I would like to show the inheritance chain of the `tensor` class. In many deep learning codebases, you will notice multi-level inheritance. Using the same technique shown below, you can find out the inheritance chain of such classes, which can help you better understand what's going on under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2d6ad-e9ad-4f25-ae53-ae37305ef005",
   "metadata": {},
   "source": [
    "# Inheritance Chain\n",
    "\n",
    "Let's see the inheritance chain of `tensor` and `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6676b0c6-9c74-4d76-8ed9-852dc4cc8e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor inheritance: [<class 'torch.Tensor'>, <class 'torch._C.TensorBase'>, <class 'object'>]\n",
      "ndarray inheritance: [<class 'numpy.ndarray'>, <class 'object'>]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Inheritance\n",
    "print(f\"tensor inheritance: {torch.tensor([]).__class__.mro()}\")\n",
    "print(f\"ndarray inheritance: {np.array([]).__class__.mro()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b042a41-217f-427f-aea7-bfe32728e724",
   "metadata": {},
   "source": [
    "Now, the first thing to learn about any python class is to learn how to initialize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f380b-9ace-41ac-8d4a-d08b9fc13775",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cb184-6895-4319-a87b-0e226fb3956f",
   "metadata": {},
   "source": [
    "Most of the class has `__init__` method for this purpose and they will ideally have some helper docstring which can be accessed using `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0feb0073-b1db-446e-b084-2c0f64261d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mDocstring:\u001b[39m\n",
       "tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
       "\n",
       "Constructs a tensor with no autograd history (also known as a \"leaf tensor\", see :doc:`/notes/autograd`) by copying :attr:`data`.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "    When working with tensors prefer using :func:`torch.Tensor.clone`,\n",
       "    :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for\n",
       "    readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to\n",
       "    ``t.detach().clone()``, and ``torch.tensor(t, requires_grad=True)``\n",
       "    is equivalent to ``t.detach().clone().requires_grad_(True)``.\n",
       "\n",
       ".. seealso::\n",
       "\n",
       "    :func:`torch.as_tensor` preserves autograd history and avoids copies where possible.\n",
       "    :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array.\n",
       "\n",
       "Args:\n",
       "    data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
       "        NumPy ``ndarray``, scalar, and other types.\n",
       "\n",
       "Keyword args:\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, infers data type from :attr:`data`.\n",
       "    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor\n",
       "        then the device of data is used. If None and data is not a tensor then\n",
       "        the result tensor is constructed on the current device.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
       "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
       "\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
       "    tensor([[ 0.1000,  1.2000],\n",
       "            [ 2.2000,  3.1000],\n",
       "            [ 4.9000,  5.2000]])\n",
       "\n",
       "    >>> torch.tensor([0, 1])  # Type inference on data\n",
       "    tensor([ 0,  1])\n",
       "\n",
       "    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
       "    ...              dtype=torch.float64,\n",
       "    ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device\n",
       "    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
       "\n",
       "    >>> torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor\n",
       "    tensor(3.1416)\n",
       "\n",
       "    >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
       "    tensor([])\n",
       "\u001b[31mType:\u001b[39m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9f4e0-35f7-4d29-84cd-a3cdf2c4e813",
   "metadata": {},
   "source": [
    "For now, let's just focus on the `data` parameter which takes numerical data in some std form like `list`, `tuple`, `int`, `float`, `np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "761b5374-135a-4abe-a28f-2c61fde5f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) torch.float32 cpu\n",
      "tensor(1) torch.int64 cpu\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "print(x, x.dtype, x.device)\n",
    "\n",
    "x = torch.tensor(1)\n",
    "print(x, x.dtype, x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7286f-865d-4298-8206-a426b1950424",
   "metadata": {},
   "source": [
    "By default, `data` is stored in the cpu and the `dtype` is automatically inferred based on the element stored. It is often a good practice to specify these to reduce unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33f01d03-3953-44f7-9d1d-1867be43c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10., device='mps:0')\n",
      "tensor([1, 2, 3])\n",
      "tensor([1., 2., 3.])\n",
      "tensor(1.)\n",
      "tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(10.0, dtype=torch.float32, device=torch.device('mps:0')))\n",
    "print(torch.tensor([1, 2, 3]))\n",
    "print(torch.tensor((1, 2, 3), dtype=torch.float32))\n",
    "print(torch.tensor(1.0))\n",
    "print(torch.tensor(np.array(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d47be60-d4a4-4eb0-b520-3191d5927351",
   "metadata": {},
   "source": [
    "Instead of using the `__init__` method, there are several other `torch` APIs that can help instantiate `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee8bd427-8117-4574-9b96-5b6c55125590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([ 0.1218,  0.5997,  0.7335,  0.3472, -0.1635,  0.5211,  1.0149, -0.6709,\n",
      "        -1.7268,  0.7381])\n",
      "tensor([4, 9, 3, 0, 3, 8, 3, 5, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.arange(0, 10, 2))\n",
    "print(torch.linspace(0, 10, 11))\n",
    "print(torch.zeros(10))\n",
    "print(torch.ones((2, 2)))\n",
    "print(torch.randn(10))\n",
    "print(torch.randint(0, 10, size=(10,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3118bd1-a7dd-4abb-b2d9-f0902f67cfcf",
   "metadata": {},
   "source": [
    "Feel free to use `?` to check the helper docstring.\n",
    "\n",
    "Once the tensor is created, they also have some properties that are useful to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1a22f-ed4c-4f5d-89b3-5081f15e4db1",
   "metadata": {},
   "source": [
    "# Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7e57003a-0c81-4c70-9fda-b944ece4919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3673, -0.6593],\n",
      "        [-0.1490,  0.1224],\n",
      "        [ 1.1192, -0.5757],\n",
      "        [ 1.4655, -0.1193]])\n",
      "8\n",
      "torch.Size([4, 2])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn((4, 2)) # [0, 1, 2, ..., 9]\n",
    "print(t)\n",
    "print(t.numel()) # Number of elements\n",
    "print(t.shape) # Shape of the tensor\n",
    "print(len(t.shape)) # Order of the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcfd47-14b8-421d-aba8-7e4446a62625",
   "metadata": {},
   "source": [
    "Now that we know how to create a `tensor` and how to access its properties. The next thing to look into is how to extract an element or a slice from an existing `tensor`. This is also called *indexing* / *slicing*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afd2a43-75ef-47f2-8fa1-c82195489298",
   "metadata": {},
   "source": [
    "# Indexing / Slicing\n",
    "\n",
    "`tensor` has same way of indexing as python `list` i.e. using the bracket [] notation. The index count starts from 0 and not 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e9642c2b-1823-419f-a010-2cd00f47f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8268, -0.7101,  1.6207, -0.9045, -0.8997, -0.4761,  0.4710,  0.4985,\n",
      "        -1.3404,  1.1091])\n",
      "tensor([ 0.8268, -0.7101,  1.6207, -0.9045, -0.8997, -0.4761,  0.4710,  0.4985,\n",
      "        -1.3404,  1.1091])\n",
      "tensor(-0.9045)\n",
      "tensor(0.8268)\n",
      "tensor([-0.9045, -0.8997, -0.4761,  0.4710,  0.4985, -1.3404,  1.1091])\n",
      "tensor([ 0.8268, -0.7101,  1.6207])\n",
      "tensor([ 0.8268, -0.7101,  1.6207, -0.9045, -0.8997])\n",
      "tensor([ 0.8268, -0.7101,  1.6207, -0.9045, -0.8997, -0.4761])\n",
      "tensor([ 0.4985, -1.3404,  1.1091])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "print(x)\n",
    "print(x[:])\n",
    "print(x[3]) # 4th element\n",
    "print(x[0]) # 1st element\n",
    "print(x[3:]) # 4th element to the end\n",
    "print(x[:3]) # 1st element to 3rd (index 3 not included)\n",
    "print(x[0:5])\n",
    "print(x[:-4]) # - is shorthand for len(x) - 4\n",
    "print(x[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645c7dc-ef04-42fd-a62b-f6c5f1e6559f",
   "metadata": {},
   "source": [
    "For `tensor` of higer order, you can use comma for each of the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c06566-4ddf-4158-a17e-87cc45f07067",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "Indexing will always return a `tensor` object even if it is single value. You can use `.item()` method to return a python int/float.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "039234a7-5b07-4346-8160-054e33209787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.9045), -0.9045100808143616)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3], x[3].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e346e6-b21c-410e-a329-7565af7d3044",
   "metadata": {},
   "source": [
    "So we can now access element/slice from a `tensor`, we should know how to update their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0725cd-112c-47c8-85d1-9c30c4dbc2c0",
   "metadata": {},
   "source": [
    "# Update Tensor Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f98a47d0-23d0-4697-a04d-ad8af67be142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4537, 0.4553, 0.9321, 0.3476, 0.2939, 0.2094, 0.2888, 0.6108, 0.8624,\n",
      "        0.7129])\n",
      "tensor([ 0.4537,  0.4553,  0.9321, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
      "        10.0000, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "T = torch.rand(10)\n",
    "print(T)\n",
    "\n",
    "T[3:] = 10.0\n",
    "\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45793ff0-6a4a-411d-b908-4f19f71e1647",
   "metadata": {},
   "source": [
    "# Operations\n",
    "\n",
    "Transformation simply means to take `tensor` object(s) and returns a new tensor (transformed tensor).\n",
    "\n",
    "Performing `tensor` operation is much faster due to C/C++ loop and GPU acceleration than using python `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6c79160-012e-4e84-8203-68a7245daf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 0.0000,  0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794,  0.6570,\n",
      "         0.9894,  0.4121])\n",
      "tensor([ 1.0000,  0.5403, -0.4161, -0.9900, -0.6536,  0.2837,  0.9602,  0.7539,\n",
      "        -0.1455, -0.9111])\n",
      "tensor([ 0.0000,  1.5574, -2.1850, -0.1425,  1.1578, -3.3805, -0.2910,  0.8714,\n",
      "        -6.7997, -0.4523])\n",
      "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
      "        4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03])\n",
      "tensor([  -inf, 0.0000, 0.6931, 1.0986, 1.3863, 1.6094, 1.7918, 1.9459, 2.0794,\n",
      "        2.1972])\n",
      "tensor([  -inf, 0.0000, 0.3010, 0.4771, 0.6021, 0.6990, 0.7782, 0.8451, 0.9031,\n",
      "        0.9542])\n"
     ]
    }
   ],
   "source": [
    "# Unary ops\n",
    "t = torch.arange(10)\n",
    "print(t)\n",
    "print(torch.sin(t))\n",
    "print(torch.cos(t))\n",
    "print(torch.tan(t))\n",
    "print(torch.exp(t))\n",
    "print(torch.log(t)) # base is `e`\n",
    "print(torch.log10(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70bbf97f-950e-489b-af5c-715213d96a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor([3, 4, 3])\n",
      "tensor([4, 6, 6])\n",
      "tensor([-2, -2,  0])\n",
      "tensor([3, 8, 9])\n",
      "tensor([0.3333, 0.5000, 1.0000])\n",
      "tensor([ 1, 16, 27])\n",
      "tensor([0, 0, 1])\n",
      "tensor([1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# Binary ops\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([3, 4, 3])\n",
    "\n",
    "# Arithmetic\n",
    "print(t1, t2)\n",
    "print(t1 + t2) \n",
    "print(t1 - t2)\n",
    "print(t1 * t2)\n",
    "print(t1 / t2)\n",
    "print(t1 ** t2)\n",
    "print(t1 // t2)\n",
    "print(t1 % t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "125431f8-7e08-4d00-bc95-0253f50ef00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor([3, 4, 3])\n",
      "tensor([False, False, False])\n",
      "tensor([False, False,  True])\n",
      "tensor([ True,  True, False])\n",
      "tensor([True, True, True])\n",
      "tensor([False, False,  True])\n",
      "tensor([ True,  True, False])\n"
     ]
    }
   ],
   "source": [
    "# Comparison\n",
    "print(t1, t2)\n",
    "print(t1 > t2)\n",
    "print(t1 >= t2)\n",
    "print(t1 < t2)\n",
    "print(t1 <= t2)\n",
    "print(t1 == t2)\n",
    "print(t1 != t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "204add1c-5605-4631-bcf4-653230819947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor([3, 4, 3])\n",
      "tensor([1, 2, 3, 3, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "print(t1, t2)\n",
    "print(torch.cat((t1, t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b751db9-e914-4e86-a853-408defef24ab",
   "metadata": {},
   "source": [
    "So far, we have seen that most tensor operations are performed elementwise. In the case of a unary operation, each element of a tensor is transformed independently. In the case of a binary operation, the transformation is applied to corresponding elements from two tensors of the same shape. However, there are situations where operations can still be applied to tensors of different shapes. The mechanism that makes this possible is called *broadcasting*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53b3f4-4941-486e-b2d8-4e5013605637",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "\n",
    "*Broadcating* is an implicit intermediate step where one or both tensors are expanded to compatible shapes. This hidden step is the source of many subtle and hard-to-debug errors if broadcasting rules are not properly understood.\n",
    "\n",
    "Here is the broadcating rule: \n",
    "1. Align shapes from the right\n",
    "2. For each dimension: Dimensions are compatible if they are equal, or one of them is 1\n",
    "3. If compatible, the tensor with size 1 is broadcast (virtually repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f63f66b7-8ce1-480f-9bcb-068fd1f068b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([[1, 2, 1, 0],\n",
      "        [2, 3, 1, 2]])\n",
      "t2: tensor([[1, 2, 0, 0]])\n",
      "t3: tensor([[2],\n",
      "        [1]])\n",
      "t1 + t2: tensor([[2, 4, 1, 0],\n",
      "        [3, 5, 1, 2]])\n",
      "t1 + t3: tensor([[3, 4, 3, 2],\n",
      "        [3, 4, 2, 3]])\n",
      "t1 + t4: tensor([[4, 5, 2, 1],\n",
      "        [5, 6, 2, 3]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mt1 + t3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39mt3\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mt1 + t4: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39mt4\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mt1 + t5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mt1\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mt5\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# [Ankit Anand]\n",
    "# Broadcasting\n",
    "# =======================================\n",
    "\n",
    "# Example 1\n",
    "t1 = torch.randint(0, 5, (2, 4)) # 2X4\n",
    "t2 = torch.randint(0, 5, (1, 4)) # 1X4\n",
    "t3 = torch.randint(0, 5, (2, 1)) # 2X1\n",
    "t4 = torch.randint(0, 5, (4, )) # 4\n",
    "t5 = torch.randint(0, 5, (4, 1)) # 4X1\n",
    "\n",
    "print(f\"t1: {t1}\")\n",
    "print(f\"t2: {t2}\")\n",
    "print(f\"t3: {t3}\")\n",
    "\n",
    "\"\"\" \n",
    "Mechanism\n",
    "\n",
    "====== t1 + t2 ======\n",
    "t1: 2 4\n",
    "t2: 1 4 <- t1.shape[-1] = 4 matches with t2.shape[-1] = 4\n",
    "t2: 1 4 <- t1.shape[-2] = 2 does not match with t2.shape[-2] = 1 (but it is 1 so possibility of broadcast)\n",
    "t2: 2 4 <- Virtually repeat along vertically to make it 2 4 => Apply binary operation!!\n",
    "\n",
    "====== t1 + t3 ======\n",
    "t1: 2 4\n",
    "t3: 2 1 => Virtually repeat horizontally to make it 2 4 => Apply binary operation!!\n",
    "\n",
    "====== t1 + t4 ======\n",
    "t1: 2 4\n",
    "t4:   4 <- Matches\n",
    "t4: 1 4 <- Understood as 1\n",
    "t4: 2 4 <- Virtually Repeat => Apply binary operation!!\n",
    "\n",
    "====== t1 + t5 ======\n",
    "t1: 2 4\n",
    "t5: 4 1 <- 1 does not match 4 (but it is one so repeat)\n",
    "t5: 4 4 <- Shapes do not match => Throw error\n",
    "\"\"\"\n",
    "print(f\"t1 + t2: {t1 + t2}\")\n",
    "print(f\"t1 + t3: {t1 + t3}\")\n",
    "print(f\"t1 + t4: {t1 + t4}\")\n",
    "print(f\"t1 + t5: {t1 + t5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cdd21-c4af-42f2-9ba1-1d5a28f1dcd4",
   "metadata": {},
   "source": [
    "Another very important operation is called `reshape` which is heavily used in the deep learning architectures. Thus it is important to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2964044-c993-4f5d-bba6-24a5f824b025",
   "metadata": {},
   "source": [
    "# Reshape\n",
    "\n",
    "Say, you have a $2 \\times 3$ tensor $t$ and you want to reshape it to $3 \\times 2$. This is only possible if they have same number of elements (`.numel()`).\n",
    "\n",
    "The tensor to be reshaped is first interpreted as a one-dimensional sequence in row-major (C-style) order:\n",
    "$[t[0,0], t[0,1], t[0,2], t[1,0], t[1,1], t[1,2]]$\n",
    "\n",
    "The values are then placed sequentially (row-major) into the new $3 \\times 2$ shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43a91fa7-33f1-44a5-8576-f011e118d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7100, 0.1680, 0.1622],\n",
      "        [0.3576, 0.7814, 0.7145]])\n",
      "tensor([[0.7100, 0.1680],\n",
      "        [0.1622, 0.3576],\n",
      "        [0.7814, 0.7145]])\n",
      "tensor([[0.7100, 0.1680],\n",
      "        [0.1622, 0.3576],\n",
      "        [0.7814, 0.7145]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand((2, 3))\n",
    "\n",
    "print(t)\n",
    "print(t.reshape(3, 2))\n",
    "print(t.reshape(3, -1)) # It automatically estimates the shape based on numel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a904cb-be1f-4d05-b036-19d897d0f487",
   "metadata": {},
   "source": [
    "# Inplace\n",
    "\n",
    "This is something very subtle yet very important. To understand, let's make some quick observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e25f4ae7-8ff9-44e0-bcce-6aa1bb42d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649866096 4761591088\n",
      "4649868880 4761591088\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# [Ankit Anand]\n",
    "# Inplace operation\n",
    "# =======================================\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "print(id(t1), id(t2))\n",
    "\n",
    "t1 = t1 + t2\n",
    "print(id(t1), id(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386b665-f408-4b09-b352-8827e474fe92",
   "metadata": {},
   "source": [
    "You can observe that `t1` points to different memory address before and after the operation. This suggest that the operation was not inplace and has allocated another memory block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bfb07c77-3f2d-4533-ae89-e898d991b9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4649866096 4649868880\n",
      "4649866096 4649868880\n",
      "4761415472 4761591088\n",
      "4761415472 4761591088\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "print(id(t1), id(t2))\n",
    "\n",
    "t1[:] = t1 + t2\n",
    "print(id(t1), id(t2))\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "print(id(t1), id(t2))\n",
    "\n",
    "t1 += t2\n",
    "print(id(t1), id(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5000f-603f-4b29-8493-3f839ee94296",
   "metadata": {},
   "source": [
    "Here you can observe that `t1` points to the same memory address before and after. This is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93320030-27be-41cc-962d-e812c022bf96",
   "metadata": {},
   "source": [
    "# ndarray $\\rightarrow$ tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9336ea1-50a7-4d03-b5b1-e24ec4d6a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] tensor([2, 2, 3])\n",
      "[2 2 3] tensor([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "ndarray = np.array([1, 2, 3])\n",
    "\n",
    "t_from_ndarray1 = torch.tensor(ndarray)\n",
    "t_from_ndarray1[0] = 2\n",
    "print(ndarray, t_from_ndarray1)\n",
    "\n",
    "t_from_ndarray2 = torch.from_numpy(ndarray) # Shares same underlying memory, meaning changing the tensor elements will change ndarray element\n",
    "t_from_ndarray2[0] = 2\n",
    "print(ndarray, t_from_ndarray2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ed285-45ee-4b51-abfd-683944b0a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "draft",
   "language": "python",
   "name": "draft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
